\section{Conclusion and Future Work}
\paragraph{Summary of Results}
The main goal of this thesis was to explore deterministic and stochastic schemes for solving unconstrained optimization problems. One of the methods discussed was batch gradient descent (BGD), which proved to be especially useful for minimizing convex functions. From the convergence analysis of BGD with a fixed step size, it was found that for $\alpha$-strongly convex and $L$-smooth functions, the step size $\tau$ should be chosen to be approximately $\max\{2\alpha/L^{2}, \text{ } 2/(\alpha + L)\}.$ When the function was not strongly convex or when it was difficult to determine its convexity, it was determined that the step size should be chosen to be at most $\tau=1/L.$ In this case, BGD yielded a slower running time of $O(1/k)$. However, when the strong convexity assumption was present, BGD's running time was faster, at $O(C^{k}),$ where $C\in(0,1)$. Both BGD with a fixed step size and BGD with backtracking line search were applied to a quadratic strongly convex function. We found that the variant using line search outperformed the one with a fixed step size, albeit not by a significant margin. It was also revealed that BGD could be applied to non-convex functions. However, due to its reliance on gradient information, it can easily become trapped in a local minimum, thus failing to locate a global minimum. This study primarily focused on understanding and implementing a stochastic variant of gradient descent, known as Stochastic Gradient Descent (SGD). SGD is specifically designed to minimize large-scale non-convex cost functions that arise in machine learning. Given the gradient-based nature of SGD, like BGD, it is susceptible to getting stuck in local minima. In this study, this was observed to be a potentially beneficial property for machine learning models, as in many cases, a good local minimum can provide a satisfactory solution.  Moreover, a model that achieves the global minimum on training data might not generalize well to new data due to overfitting, whereas a local minimum could strike a better balance between fitting the data and generalizing to new examples as discussed in \cite[282-290]{Goodfellow-et-al-2016}. In machine learning, SGD often outperforms BGD for non-convex cost functions. This is because SGD can escape shallow local minima, while BGD tends to get stuck in the first local minimum it encounters. Furthermore, if the size of the training dataset $n$ is too large, it is impractical to compute the full gradient at each iteration. Then SGD is favored over BGD regardless of the function's convexity. The only time that BGD is preferred over SGD is when dealing with a not-too-large dataset and a convex cost function. In this study, SGD was applied to the Mean Squared Error (MSE) cost function given in Equation \eqref{MSE_cost_fun}. This function was computed based on labeled data of handwritten digits and outputs from a deep neural network designed to predict the labels of these digits. By employing SGD on the MSE cost function, a more optimal set of parameters was determined, which improved the neural network's ability in classifying the handwritten digits. It is noteworthy that SGD is suitable for cost functions in machine learning, as they typically take the form of a sum of functions. However, many non-convex functions like Ackley's function, given in equation \eqref{eq:18}, cannot be written as a sum of functions. In these situations, BGD outperforms SGD, simply because SGD isn't applicable. Global optimization methods become the preferred choice for non-convex functions like Ackley's function, where gradient-based methods aren't effective. Unlike gradient-based methods, these approaches, such as Simulated Annealing (SA) and Particle Swarm Optimization (PSO), employ diverse search strategies to thoroughly explore the solution space. This increases the likelihood of identifying the global minimum, as detailed in \cite[1-5]{horst1995handbook}. In machine learning, these methods are often not preferred over SGD due to their relatively slower convergence compared to SGD.
\paragraph{Future Work}
This thesis has offered insights into various optimization techniques. Some of these techniques, like BGD, are regarded as classical approaches, while others, such as SGD, are considered innovative. This study opens several avenues for exploration. For example, one can investigate different variants of BGD. One possible variant to study is called Gradient Descent with Momentum. This method uses a momentum term in the update process, which could potentially accelerate convergence. Additionally, one can explore adaptive gradient descent algorithms like AdaGrad, RMSProp and Adam. These methods dynamically adjust the step size, which could speed up convergence. Extending these algorithms to their stochastic versions could result in SGD variants worth investigating. This study explored unconstrained optimization problems. However, it is also possible to investigate constrained problems and explore the applicability of gradient descent methods in such scenarios. Furthermore, conducting a comparative analysis of SA and PSO on a wider range of benchmark functions could be effective in understanding their respective strengths and weaknesses. Additionally, exploring the application of SA and PSO in real world-scenarios across domains such as engineering, finance, or machine learning could reveal their effectiveness for solving practical optimization tasks.