\section{Stochastic optimization}
\subsection{Minimizing Sums and Expectation}
As discussed in \cite[20]{coursenotesML}, a large class of functions in machine learning can be expressed as minimizing expectations of the form
\begin{equation*}\label{expectations}\tag{4.2.1}
\underset{x \in \mathbf{R}^{p}}{\text{min}} F(x) \overset{\text{def.}}{=} \mathbb{E}_{z\sim P_{z}}[f(x,z)] = \int_{\Omega} f(x, z) dP_z(z)
\end{equation*}
In this framework, one works in a specific probability space: $\left(\Omega, \mathcal{F}, \mathbb{P}\right),$ where $f: \mathbf{R}^{p} \times \Omega \rightarrow \mathbf{R}$ is a function depending on a random argument $z$ and a parameter $x$ to be optimized.
While minimizing expectations is a general framework that allows us to account for uncertainty in the data and model parameters, in many practical machine learning applications, we encounter optimization problems where the goal is to minimize a sum of functions of the form
\begin{equation*}\label{largesums}\tag{4.2.2}
\underset{x \in \mathbf{R}^{p}}{\text{min}} F(x) \overset{\text{def.}}{=} \frac{1}{n} \sum_{i=1}^{n}f_{i}(x)
\end{equation*}Problem \ref{largesums} can be seen as a special case of \ref{expectations} when assuming that the unknown distribution is a discrete uniform distribution. Specifically, if we let $P_{z}\sim\mathcal{U}\{1,\ldots,n\}$ and set $f(x, z_i) = f(x, i) = f_{i}(x)$, we can work with the probability mass function:
\begin{equation*}
\mathbb{P}(z = i) = \frac{1}{n}
\end{equation*}
Under this assumption, the expectation of $f(x, z)$ can be expressed as follows:
\begin{align*}
F(x)
    &= \mathbb{E}_{z\sim P_{z}}[f(x,z)] = \sum_{i=1}^{n} f(x, z_i) \mathbb{P}(z = i) = \sum_{i=1}^{n} f_{i}(x) \frac{1}{n} = \frac{1}{n} \sum_{i=1}^{n} f_{i}(x)
\end{align*}
Thus, when we assume that the unknown distribution is a discrete uniform distribution, minimizing the expectation in \ref{expectations} becomes equivalent to minimizing the sum in \ref{largesums}, where $f(x, z_i) = f_{i}(x).$ Minimizing sums with a discrete uniform distribution is a widely used technique in machine learning. This is because in many machine learning applications, explicitly specifying a probability distribution over the data can be infeasible. By assuming a discrete uniform distribution, each data point is treated as equally likely, making the optimization problem more tractable. This formulation conveniently transforms the cost function, initially defined as an expectation, into a sum, which simplifies the problem and makes it easier to work with. In general, stochastic optimization methods like SGD are not always faster than deterministic methods like the usual gradient descent. If the size of the dataset $n$ is not too large, then one can afford to perform a few deterministic iterations. Then deterministic methods can be faster. But if $n$ is too big for even one deterministic iteration, stochastic methods can help because they can break down the cost of deterministic iterations into smaller pieces, which allows for a more precise approach.

\subsection{Batch Gradient Descent (BGD)}
The usual deterministic (batch) gradient descent is studied in extensive detail in Section \ref{sec: GD}. Its iterations read $$x_{k+1} = x_{k} - \tau_{k}\nabla F(x_{k})$$
and without the strong convexity assumption, the step size should be chosen as $0 < \tau_{k} < \tau_{\text{max}} = 1/L$ where $L$ is the Lipschitz constant of the gradient $\nabla F$. \ref{pf_gd_c_L} In this deterministic setting choosing $\tau \approx \tau_{max}$ ensures quite fast convergence. (even linear rates if $f$ is strongly convex.) In the setting of minimizing large sums, the gradient reads 
\begin{equation*}\label{full_grad_of_sum}\tag{4.3.1}
\nabla F(x) = \frac{1}{n} \sum_{i=1}^{n}\nabla f_{i}(x)
\end{equation*}
So it typically has complexity $O(np)$ if computing $\nabla f_{i}(x)$ has linear complexity $p.$ Here $i \in \{1,\ldots,n\}$ represent indices and not random variables. 

\subsection{Stochastic Gradient Descent (SGD)}
In general settings, Stochastic Gradient Descent (SGD) can be applied to minimize expectations over any probability space, not only in the case of a discrete uniform distribution. This broader framework can be particularly useful when dealing with large-scale optimization problems, where computing the exact gradient can be computationally expensive. In this section, an overview of the general SGD approach will be provided, while still highlighting its applications in the context of minimizing large sums.

Given a random variable $z$ with a probability distribution $P_z$, the objective is to minimize the expected value of a function $f(x, z)$, where $x \in \mathbf{R}^p$:
\begin{equation*}\tag{4.4.1}
F(x) = \mathbb{E}_{z \sim P_z}[f(x, z)].
\end{equation*}

In contrast to the standard gradient descent method, which requires the computation of the exact gradient $\nabla F$, the SGD algorithm approximates the gradient using a single stochastic estimate. To compute this approximation, first, a random variable $z_k$ is drawn from the distribution $P_z$. This $z_k$ is an independent realization of $z$ at iteration $k$. Next, the gradient of $f(x, z_k)$ for this specific realization is calculated.

The stochastic gradient $\nabla f(x, z_k)$ is an unbiased estimator of the true gradient, which can be shown through the following expectation:
\begin{equation*}\label{gd_unbiased}\tag{4.4.2}
\mathbb{E}_{z \sim P_z}[\nabla f(x, z)] = \nabla F(x)
\end{equation*}
Here, $z$ inside the expectation denotes the random variable following the distribution $P_z$. The expectation is taken over all possible draws of the random variable $z$. It demonstrates that the expected value of the stochastic gradient computed for any draw $z_k$ from the distribution $P_z$ is equal to the true gradient $\nabla F(x)$. The $z_k$'s represent the draws from the distribution $P_z$ at iteration $k$, and they are all random variables, each being an independent realization of $z$. The subscript $k$ denotes the iteration number of the SGD, and the assumption is that the $z_k$'s are all independent and identically distributed (i.i.d.). Starting from an initial point $x_0$, the SGD iteration is performed using the stochastic gradient computed based on the specific draw $z_k$. The update equation is as follows:
\begin{equation*}\tag{4.4.3}
x_{k+1} = x_k - \tau_k \nabla f(x_k, z_k),
\end{equation*}
In this equation, $x_k$ is the current parameter estimate, $\tau_k$ is the step size at iteration $k$, and $\nabla f(x_k, z_k)$ is the stochastic gradient estimate based on the draw $z_k$ from the distribution $P_z$.  In the context of minimizing large sums with a discrete uniform distribution, the random variable $z_k$ is used to represent the index of a randomly chosen data point, and $i(k)$ is used as an equivalent notation for $z_k$. The formula becomes:
\begin{equation*}\label{SGD_update}\tag{4.4.4}
x_{k+1} = x_k - \tau_k \nabla f_{i(k)}(x_k),
\end{equation*}
In this case, $i(k)$ is a random variable drawn from the uniform distribution on $\{1, \ldots, n\}$. For each iteration index $k$, $i(k)$ represents the index of a randomly selected data point, and the gradient computation is based on this specific data point. The sequence of vectors $x_{k+1}$ forms a random process, and the convergence analysis of SGD aims to determine whether this process converges in quadratic mean to a deterministic vector $x^*$ that minimizes the objective function $F$. Note that in the case of minimizing large sums, batch gradient descent has complexity $O(np),$ while a step of SGD only has complexity $O(p).$ SGD is thus advantageous when $n$ is very large, and one cannot afford to compute the full gradient.


An essential aspect of SGD is determining an appropriate step size schedule. Adaptive step size methods are favored over fixed step sizes, as they can adjust to the characteristics of the dataset based on the gradient behavior. The step size $\tau_k$ should go to zero as $k \rightarrow \infty$, but not too quickly, as it can impact the convergence of the algorithm. As extensively covered in \cite[9-11]{murata}, a typical step size schedule that ensures both properties is to have $\tau_k \sim k^{-1}$ for $k \rightarrow \infty$. In practical applications, minimizing large sums is a common task. Employing a discrete uniform distribution to address these problems is often more convenient and widely used. Stochastic Gradient Descent (SGD) can handle both this approach and the general framework for optimization. SGD's versatility and computational efficiency make it a popular choice in machine learning, particularly for large-scale challenges.


\subsection{Machine learning}
\subsubsection{Introduction}
Machine learning is a field that enables computers to learn from data and improve their performance. One effective approach is supervised learning, which learns from labeled data. In this method, a model with parameters, denoted as $x \in \mathbf{R}^{p}$, is used, aiming to find an optimal set of parameters that work well for the given task. This performance is measured by a loss function, which tells the difference between the model's predicted and true outputs. By adjusting the model's parameters to minimize the loss function, the model's performance on the task improves. The objective or cost function is typically the sum of individual losses across a dataset. Stochastic Gradient Descent (SGD) methods are often employed when dealing with large or infinite datasets, as they are efficient optimization tools that minimize the cost function. The overall goal of supervised learning and the use of SGD is to find the set of model parameters that result in the lowest possible cost, meaning the model is performing optimally on the task at hand. In the following sections, the details of these concepts and their application in the context of neural network optimization will be discussed.
\begin{comment}
Machine learning is a powerful field that allows computers to learn from data, improving their performance on a task without being explicitly programmed. Among the various approaches in machine learning, supervised learning stands out due to its effectiveness in learning from labeled data. In supervised learning, we use a model with parameters, denoted as $x \in \mathbf{R}^{p}$, where $p$ is the number of parameters. The aim is to find an optimal set of parameters that perform well on a given task. This performance is often measured by a loss function, which quantifies the difference between the model's predicted output and the true output. The model's parameters are adjusted to minimize this loss function, thereby improving the performance of the model on the task. The objective or cost function is typically defined as the sum of the individual losses across a dataset. When the number of samples $n$ in the dataset is large, potentially even infinite, it becomes computationally expensive to compute the loss for all samples at once. In such scenarios, Stochastic Gradient Descent (SGD) methods are often employed. These methods are efficient optimization tools that enable us to minimize the cost function even when dealing with large or infinite datasets. Overall, the goal of supervised learning and the use of SGD is to find the set of model parameters that result in the lowest possible cost, meaning the model is performing optimally on the task at hand. In the following sections, we delve into the details of these concepts and their application in the context of neural network optimization.
\end{comment}
\subsubsection{Basics of Supervised Learning and Neural Networks}
\paragraph{Supervised Learning}
Supervised learning is a branch of machine learning. It operates in settings where we have a wealth of labeled data readily available. This data can be described by a function $f^*$ that maps an input $x$ to a labeled output $y=f^*(x)$ \cite{bishop2006}. Therefore, the data is a collection of input-output pairs $(x, y)$. In practice, we select a small subset of this data, known as the training data, to build a model. This model is represented by a function $f$ that maps an input $x$ and parameters $\theta$ (commonly known as weights or biases) to a predicted output $\hat{y}$ \cite{Goodfellow-et-al-2016}. The ultimate goal is to align $f$ as closely as possible with $f^*.$ The training data offers us noisy, approximate examples of $f^*(x).$ The objective of supervised learning is to adjust the model's parameters $\theta$ such that the model's predictions align closely with the actual output values from the training data. This is achieved by defining a loss function that quantifies the difference between the model's predictions and the actual output values. Optimization algorithms, like stochastic gradient descent (SGD), are then used to iteratively adjust the model's parameters to minimize this loss. The primary goal of supervised learning is to create a model that not only fits the training data well, but also generalizes effectively to the rest of the original data not included in the training set, and more importantly, to unseen data. This means that the model should be able to make accurate predictions even when presented with new data, showcasing its capacity for generalization \cite{james2013}.
\paragraph{Feed-forward Neural Networks}
In supervised learning, feed-forward neural networks approximate the mapping $f^*: x \mapsto y$ by defining a mapping $f: (x;\theta) \mapsto \hat{y}$, and iteratively learning parameter values $\theta$ for the best function approximation. The network is composed of distinct layers (input, hidden, and output layers), each with a number of neurons. For a context where $x \in \mathbf{R}^{p}$, the input layer comprises $p$ neurons. Each neuron in a layer receives input from neurons in the previous layer, applies a weighted sum with a bias term, and then applies an activation function. The activation introduces non-linearity into the model. The final prediction is made at the output layer. The goal during training is to adjust weights and biases to minimize the difference between the network's predictions and the actual values, as quantified by a loss function. An optimization algorithm, such as stochastic gradient descent, facilitates these adjustments.

\newpage
\subsubsection{Convergence Analysis of SGD general case \ref{expectations}}
In this section, a brief overview of the convergence of stochastic gradient descent (SGD) in the general case is presented, where the goal is to minimize an expectation with an unknown probability distribution. The detailed proof of this proposition will not be provided in this bachelor thesis, but its key components will be outlined, and the original proof by Turinici \cite{turinici2021convergence} will be cited. This thesis will focus on the case where the distribution is an empirical uniform measure, i.e., the case where one minimizes large sums and the distribution is uniform on $\{1,\dots,n\}$, where $n$ is the size of the dataset.
The goal of SGD is to find a minimum of $F$. It operates iteratively using the update formula:
\begin{align}
x_{k+1} = x_{k} - \tau_{k}\nabla_{x}f(x_{k}, z_{k}), \tag{2}
\end{align}
where at iteration $k$:
\begin{itemize}
\item $\tau_k$ is a deterministic "step size schedule,"
\item $z_k \in \Omega$ is a random variable independent of any previously drawn random variables.
\end{itemize}

To prove the convergence, the following hypotheses are needed:
\begin{enumerate}
\item The gradient of $f$ is bounded:
\begin{align}
\exists B > 0\colon \underset{Z}{\sup}\left\Vert\nabla_{x} f(x, z)\right\Vert^{2} \leq B, \forall x \in \mathbf{R}^{p}. \tag{3}
\end{align}
\item $F$ is strongly convex:
\begin{align}
\exists\mu > 0: F(y) \geq F(x) + \langle\nabla F(x), y-x\rangle + \frac{\mu}{2}\left\Vert x-y\right\Vert^{2}, \forall x, y \in \mathbf{R}^{p}. \tag{4}
\end{align}
\end{enumerate}
Note that for $\mu = 0$, this is just the usual convexity, i.e., the function is above its tangent. For $\mu > 0$, the function is above a parabola. Proving the convergence of $(x_{k})_{k\in \mathbb{N}}$ involves verifying each item in the corresponding proposition.
\begin{proposition}[\cite{turinici2021convergence}]\label{pf_sgd_conv_general}
Suppose that each $f(\cdot, z)$ is differentiable and that $F$ satisfies hypotheses (3) and (4). Then:\

the function $F$ has a unique minimum $x^{*}$;\
For any $k \geq 0$ denote
\begin{align}
d_{k} = \mathbb{E}[\left\Vert x_{k}-x^{*}\right\Vert^{2}]. \tag{5}
\end{align}
Then
\begin{align}
d_{k+1} \leq (1 - \tau_{k}\mu)d_{k} + \tau_{k}^{2}B. \tag{6}
\end{align}
For any $\epsilon > 0$ there exists a $\tau > 0$ such that if $\tau_{k} = \tau$ then
\begin{align}
\limsup_{n\rightarrow\infty}{\mathbb{E}[\left\Vert x_{k+1}-x^{*}\right\Vert^{2}]} \leq \epsilon. \tag{7}
\end{align}
Take sequence $(\tau_{k})_{k\in \mathbb{N}}$ such that:\
\begin{align}
\tau_{k} \rightarrow 0 \text{ } and \text{ } \sum_{k\geq 1} \tau_{k} = \infty. \tag{8}
\end{align}
Then $d_{k} = \mathbb{E}[\left\Vert x_{k}-x^{}\right\Vert^{2}] \xrightarrow{k\rightarrow\infty} 0,$ that is $\lim_{k\rightarrow\infty}x_{k} = x^{},$ where the convergence is on the $L^{2}$ norm for random variables (convergence in quadratic mean), In other words, $x_{k} \xrightarrow{L^{2}} x^{*}$.
\end{proposition}
For a comprehensive proof of this proposition, please refer to Turinici's work \cite{turinici2021convergence}.
\begin{comment}
\paragraph{Recall of the general framework} Suppose $(\Omega, \mathcal{F}, \mathbb{P})$ is a probability space, $f: \mathbf{R}^{p} \times \Omega \rightarrow \mathbf{R}$ a function depending on a random argument $z$ and a parameter $x$ to be optimized. Denote
\begin{align} 
F(x) = \mathbb{E}_{z}[f(x, z)] \tag{1}
\end{align}
The goal of SGD is to find a minimum of $F$. It operates iteratively by taking at iteration $k$:
\begin{itemize}
    \item a (deterministic) "step size schedule" $\tau_k$
    \item a random $z_k \in \Omega$ independent of any other previous random variables drawn
    \item and updating by the formula
\end{itemize}
\begin{align}
x_{k+1} = x_{k} - \tau_{k}\nabla_{x}f(x_{k}, z_{k}) \tag{2}
\end{align}
\paragraph{Hypothesis on $f$ and $F$} In order to prove the convergence we need  the following hypothesis:\\
1. The gradient of $f$ is bounded:
\begin{align}
\exists B > 0\colon \underset{Z}{\sup}\left\Vert\nabla_{x} f(x, z)\right\Vert^{2} \leq B, \forall x \in \mathbf{R}^{p}. \tag{3}
\end{align}
2. $F$ is strongly convex:
\begin{align}
\exists\mu > 0: F(y) \geq F(x) + \langle\nabla F(x), y-x\rangle + \frac{\mu}{2}\left\Vert x-y\right\Vert^{2}, \forall x, y \in \mathbf{R}^{p}. \tag{4}
\end{align}
Note that for $\mu = 0$ this is just the usual convexity, i.e. the function is above its tangent. For $\mu > 0$, the equation above implies that the function is above a parabola. Now proving convergence of $(x_{k})_{k\in \mathbb{N}}$ boils down to proving each item in the following proposition: 
\begin{proposition}\textnormal{\cite{self_contained_proof}}\label{pf_sgd_conv_general}
Suppose that each $f(\cdot, z)$ is differentiable and that $F$ satisfies hypothesis (3) and (4). Then\\
1. the function $F$ has an unique minimum $x^{*}$;\\
2. For any $k \geq 0$ denote
\begin{align}
d_{k} = \mathbb{E}[\left\Vert x_{k}-x^{*}\right\Vert^{2}]. \tag{5}
\end{align}
Then 
\begin{align}
d_{k+1} \leq (1 - \tau_{k}\mu)d_{k} + \tau_{k}^{2}B. \tag{6}
\end{align}
3. For any $\epsilon > 0$ there exists a $\tau > 0$ such that if $\tau_{k} = \tau$ then
\begin{align}
\limsup_{n\rightarrow\infty}{\mathbb{E}[\left\Vert x_{k+1}-x^{*}\right\Vert^{2}]} \leq \epsilon. \tag{7}
\end{align}
4. Take sequence $(\tau_{k})_{k\in \mathbb{N}}$ such that:\\
\begin{align}
\tau_{k} \rightarrow 0 \text{ } and \text{ } \sum_{k\geq 1} \tau_{k} = \infty. \tag{8}
\end{align}
Then $d_{k} = \mathbb{E}[\left\Vert x_{k}-x^{*}\right\Vert^{2}] \xrightarrow{k\rightarrow\infty} 0,$ that is $\lim_{k\rightarrow\infty}x_{k} = x^{*},$ where the convergence is on the $L^{2}$ norm for random variables (convergence in quadratic mean), In other words, $x_{k} \xrightarrow{L^{2}} x^{*}$.
\end{proposition}
\end{comment}
\newpage
\subsubsection{Convergence Analysis of SGD special case \ref{largesums}}
To solve the minimization problem described in \ref{largesums} by means of SGD, it is required that $\left\Vert x_{k} - x^{*}\right\Vert$ tends to zero in quadratic mean, or equivalently, $\mathbb{E}[\left\Vert x_{k} - x^{*}\right\Vert^{2}] \rightarrow 0$ as $k \rightarrow \infty,$ where $\left\Vert\cdot\right\Vert$ is the Euclidean norm. The following proposition \ref{conv_prop_sgd} provides a theoretical foundation in which SGD is ensured to converge. 
\begin{proposition}\label{conv_prop_sgd}
Let $f: \mathbf{R}^{p} \longrightarrow \mathbf{R}$ be $\mu$-strongly convex and differentiable on $\textbf{dom}f = \mathbf{R}^{p}$, and is such that $\left\Vert\nabla f_{i}(x)\right\Vert^{2} \leq C^{2}.$ For the step size choice $\tau_{k} = \frac{1}{\mu (k+1)}$, one has
\begin{equation*}\label{bound_conv_SGD}\tag{4.4.1.1}
\mathbb{E}[\left\Vert x_{k} - x^{*}\right\Vert^{2}] \leq \frac{R}{k+1}\text{ }\text{ } where \text{ }\text{ } R = \textnormal{max}\left\{\left\Vert x_{0} - x^{*}\right\Vert^{2},\text{ } C^{2}/\mu^{2}\right\},
\end{equation*}
where $\mathbb{E}$ indicates an expectation with respect to the i.i.d. sampling performed at each iteration.
\end{proposition}
\begin{proof} By strong convexity, one has
\begin{align}
    &f(y) - f(x) \geq \langle \nabla f(x),\text{ } y-x\rangle + \frac{\mu}{2}||y-x||^{2} \text{ }\text{ } \forall x,y \in \mathbf{R}^{p} \text{,} \text{ } \text{ } \text{hence } \nonumber\\
    &f(x_{k}) - f(x^{*}) \geq \langle \nabla f(x^{*}),\text{ } x_{k}-x^{*}\rangle + \frac{\mu}{2}||x_{k}-x^{*}||^{2}\nonumber\\
    &f(x^{*}) - f(x_{k}) \geq \langle \nabla f(x_{k}),\text{ } x^{*}-x_{k}\rangle + \frac{\mu}{2}||x^{*}-x_{k}||^{2}.\nonumber
\end{align}
Summing these two inequalities and using $\nabla f(x^{*}) = 0$ leads to
\setcounter{equation}{0}
\begin{align}
    &\langle \nabla f(x_{k}) - \nabla f(x^{*}),\text{ } x_{k} - x^{*}\rangle = \langle \nabla f(x_{k}),\text{ } x_{k} - x^{*}\rangle \geq \mu ||x_{k} - x^{*}||^{2}.
\end{align}
Considering only the expectation with respect to the random sample of $i(k) \sim \textbf{i}_{k}$, one has
\begin{align}
\mathbb{E}_{\textbf{i}_{k}}[||x_{k+1}-x^{*}||^{2}]
    &= \mathbb{E}_{\textbf{i}_{k}}[||x_{k} - \tau_{k}\nabla f_{\textbf{i}_{k}}(x_{k}) - x^{*}||^{2}]\nonumber\\
    &= \mathbb{E}_{\textbf{i}_{k}}[||x_{k}-x^{*}||^{2} + 2\tau_{k}\langle\nabla f_{\textbf{i}_{k}}(x_{k}),\text{ } x^{*} - x_{k}\rangle + \tau_{k}^{2}||\nabla f_{\textbf{i}_{k}}(x_{k})||^{2}]\nonumber\\
    &= ||x_{k}-x^{*}||^{2} + 2\tau_{k}\langle \mathbb{E}_{\textbf{i}_{k}}[\nabla f_{\textbf{i}_{k}}(x_{k})], \text{ } x^{*} - x_{k}\rangle + \tau_{k}^{2} \mathbb{E}_{\textbf{i}_{k}}[||f_{\textbf{i}_{k}}(x_{k})||^{2}]\\
    &\leq ||x_{k}-x^{*}||^{2} + 2\tau_{k}\langle \nabla f(x_{k}), \text{ } x^{*} - x_{k}\rangle + \tau_{k}^{2} C^{2}\\
    &\leq ||x_{k}-x^{*}||^{2} - 2\mu \tau_{k} ||x_{k}-x^{*}||^{2} + \tau_{k}^{2} C^{2}.
\end{align}
To go from step (2) to (3), the fact that the gradient is unbiased \ref{gd_unbiased} and the assumption that $\left\Vert\nabla f_{i}(x)\right\Vert^{2} \leq C^{2}$ is used. To go from step (3) to (4), inequality (1) is used. Taking now the full expectation with respect to all the other previous iterates, one obtains
\begin{align} 
    &\mathbb{E}[||x_{k+1}-x^{*}||^{2}]\leq \mathbb{E}[||x_{k}-x^{*}||^{2}] - 2\mu \tau_{k} \mathbb{E}[||x_{k}-x^{*}||^{2}] + \tau_{k}^{2} C^{2} = (1 - 2\mu \tau_{k}) \mathbb{E}[||x_{k}-x^{*}||^{2}] + \tau_{k}^{2} C^{2}\nonumber
    \hspace{0.075cm} \text{\scalebox{0.925}{(5)}}
\end{align}
By induction, the bound in \ref{bound_conv_SGD} can be shown. Denote $\varepsilon_{k} \overset{\scalebox{0.5}{def.}}{=} \mathbb{E}[||x_{k}-x^{*}||^{2}]$. Indeed, for $k = 0$, it holds $$\varepsilon_{0} = \mathbb{E}[||x_{0}-x^{*}||^{2}] \leq \frac{\textnormal{max}\left\{\left\Vert x_{0} - x^{*}\right\Vert^{2},\text{ } C^{2}/\mu^{2}\right\}}{1} = \frac{R}{1}$$
The induction hypothesis reads: $\varepsilon_{k} \leq \frac{R}{k+1}.$ Using inequality (5) with $\tau_{k} = \frac{1}{\mu(k+1)},$ and denoting $m = k+1$
\begin{align}
\varepsilon_{k+1}
    &\leq (1-2\mu \tau_{k}) \varepsilon_{k} + \tau_{k}^{2} C^{2} = \left(1-\frac{2}{m}\right)\varepsilon_{k} + \frac{C^{2}}{(\mu m)^{2}}\nonumber\\
    &\leq \left(1 - \frac{2}{m}\right)\frac{R}{m} + \frac{R}{m^{2}} = \left(\frac{1}{m} - \frac{1}{m^{2}}\right)R = \frac{m-1}{m^{2}}R = \frac{m^{2} - 1}{m^{2}}\frac{1}{m + 1}R \leq \frac{R}{m+1}\nonumber
\end{align}
Hence the bound holds for all $k \in \mathbb{N}.$ Now since, $0 \leq \mathbb{E}[\left\Vert x_{k} - x^{*}\right\Vert^{2}] \leq \frac{R}{k+1} \xrightarrow{k \rightarrow 0} \infty$. It follows from the squeeze theorem\footnote{See \ref{squeeze_theorem} for the squeeze theorem.} that $\mathbb{E}[\left\Vert x_{k} - x^{*}\right\Vert^{2}] \xrightarrow{k \rightarrow \infty} 0.$ In other words, $(x_{k})_{k\in\mathbb{N}}$ converges on the $L^{2}$ norm for random variables with $x_{k} \xrightarrow{L^{2}} x^{*}.$ 
\end{proof}

\subsection{Pros and Cons of Stochastic Gradient Descent}
One of the weaknesses of SGD is that it only weakly benefits from strong convexity. Whereas BGD enjoys a fast linear rate for strongly convex functions. In BGD, the entire dataset is used to compute the gradient at each iteration, therefore the algorithm can take advantage of the strong curvature of the function and make a large step towards the minimum. On the other hand, in SGD, since only a subset of the data is used to compute the gradient, the gradient estimate is less accurate, and the algorithm may not be able to take advantage of the strong curvature of the function.

\subsection{Brief Overview of Stochastic Global Optimization Algorithms}
\subsubsection{Introduction}
In the context of global optimization methods, the goal of solving the unconstrained minimization problem described in \ref{eq:8} remains the same. While local optimization methods like gradient descent are effective for convex functions, they can struggle with non-convex functions due to their reliance on gradient information, which may lead them to become trapped in local minima. Stochastic Gradient Descent (SGD) is often sufficient in machine learning settings where a local minimum can provide a satisfactory solution and balance between fitting the data and generalizing to new examples, as discussed in \cite[282-290]{Goodfellow-et-al-2016}.

Global optimization methods serve as an alternative to gradient-based optimization techniques, aiming to find a global minimum. These methods are especially important for solving non-convex functions where local optimization methods may not perform well. In contrast to local optimization methods, global optimization methods employ various search strategies to explore the solution space more thoroughly, thereby increasing the likelihood of identifying the global optimum, as discussed in \cite[1-5]{horst1995handbook}. In this section, two popular global optimization methods: Simulated Annealing and Particle Swarm Optimization (PSO) will be discussed.

\subsubsection{Simulated Annealing}
\paragraph{History and Motivation}
The Simulated Annealing algorithm is a stochastic global optimization method that efficiently searches for the global minimum of a non-convex function. These functions typically arise from problems in physics, chemistry and biology and are called energy functions \cite[1]{XIANG1997216}. The key feature of simulated annealing is that it provides a means to escape local minima by allowing hill-climbing moves (i.e., moves which worsen the objective function value) in hopes of finding a global minimum \cite{Henderson2003}. The analogy of simulated annealing is that it is similar to the process of blacksmiths making a sword. In order to make a strong and sharp sword, the molecules of the metal must be aligned in the same way. Blacksmiths achieve this by repeatedly heating and cooling the metal to give the molecules a chance to realign themselves into the lowest energy state. This is called annealing. Similarly, the simulated annealing optimization algorithm finds the optimal solution by randomly exploring the solution space and gradually reducing the "temperature" or the randomness in the search process to converge to the optimal solution. So, just as blacksmiths use annealing to create a strong sword, simulated annealing uses a similar approach to find the optimal solution in a search space. 
\paragraph{General Setting}
To describe the features of the simulated annealing algorithm, several definitions are needed. These definition are for the most part identical to the definitions used in gradient-based optimization methods. Let $\left(\Omega,\left\Vert\cdot\right\Vert_{\Omega}\right)$ be the solution space (i.e., the set of all possible solutions) . Let $f: \Omega \rightarrow \mathbf{R}$ be an objective function (possible non-convex) defined on the solution space. The goal is to find an optimal solution, $x^{*}$ such that $f(x) \geq f(x^{*}) \text{ }\forall x \in \Omega.$ The objective function must be bounded to ensure that $x^{*}$ exists. Furthermore, define $S = \{x + \varepsilon\cdot\xi \text{ }|\text{ } \xi_{i} \in \left(-1, 1\right) \text{ } \forall i\}$ to be the neighborhood function for $x \in \Omega$ where $\varepsilon > 0$ is arbitrary. So, associated with every solution, $x \in \Omega$, are neighboring solutions in $S$ that can be reached in a single iteration of the algorithm \cite[2]{metropolis1953equation}. The set $S$ represents a region in $\Omega$ obtained by adding small displacements to $x$ along each coordinate axis. In case $\Omega = \mathbf{R}^n$, the set represents an n-dimensional hypercube with $x$ as its origin and side lengths $2\varepsilon.$ From now on we assume that $\Omega = \mathbf{R}^n.$ Simulated annealing starts with a starting vector $x \in \Omega.$ A neighboring solution $x' \in S$ is then generated using the rule described in \cite[2]{metropolis1953equation}. 
\newpage 
Namely according to the following pre-specified rule:
\begin{equation}\tag{4.6.2.1}
x' = x + \varepsilon\cdot\xi  
\end{equation}
where $\varepsilon$ is the maximum allowed displacement, which for the sake of this argument is arbitrary, and $\xi$ is a random vector with $\xi_{i} \in \left(-1,1\right)\text{ } \forall i \in \{1,\ldots,n\}.$ Simulated annealing is based on the Metropolis acceptance criterion \parencite[2]{metropolis1953equation}, which models how a thermodynamic system moves from the current state $x$ to the next state $x'$, intending to minimize the energy content. The candidate $x'$ is accepted as the new solution based on the acceptance probability provided in \cite[289]{Henderson2003}:
\begin{equation}\tag{4.6.2.2}
\mathbb{P}\{\text{Accept $x'$ as next solution}\} = 
    \begin{cases}
        \exp\left[-\left(f(x')-f(x)\right)/t_{k}\right] \hspace{0.5cm}\text{if $f(x')-f(x) > 0$}\\
        1 \text{\hspace{4.1cm} if $f(x')-f(x) \leq 0$}
    \end{cases}
\end{equation}
Define $t_{k}$ as the temperature parameter at iteration $k$, such that
\begin{equation}\tag{4.6.2.3}
t_k > 0 \hspace{0.33em}\text{for all}\hspace{0.33em} k \hspace{0.33em}\text{and}\hspace{0.23em}\lim_{k\rightarrow\infty}t_{k} = 0
\end{equation}
Now as $t_{k}$ decreases $\exp\left[-\left(\Delta f\right)/t_{k}\right]$ decreases as well, lowering the probability that new solutions ($x_{new}$) are accepted in case  $\Delta f = f(x_{new})-f(x_{old}) > 0.$ In other words, when the temperature is lower, the algorithm is less likely to accept a worse solution. If the temperature schedule $t_{k}$ is chosen such that temperature reduction is sufficiently slow, a steady progression towards an optimal solution can be achieved \cite[289]{Henderson2003}. A typical temperature schedule is $t_{k+1} = \alpha t_{k}$ $\implies$ $t_{k} = \alpha^{k} t_{0},$ where $t_{0} > 0$ is the chosen starting temperature and $\alpha \in (0, 1)$ is the cooling constant which indicates the rate of change. For the best performance $\alpha$ should be chosen close to 1, e.g., $\alpha=0.99.$ A typical stopping criterion is $k = k_{max}$ (i.e., whenever the algorithm reaches a predefined maximum number of iterations).
\paragraph{Statement of Algorithm}
As stated in \cite[290]{Henderson2003}, the simulated annealing algorithm consists of the following stages:
\begin{algorithm}
\caption{\textit{Simulated Annealing Algorithm}}\label{Pseudocode_SA}
\begin{algorithmic}
\State \textbf{Select} an initial solution $x \in \Omega$.
\State \textbf{Select} the temperature change counter $k=0$
\State \textbf{Select} a temperature cooling schedule, $t_{k}$ 
\State \textbf{Select} an initial temperature $T = t_{0} \geq 0$ 
\vspace{0.03cm}
\State \textbf{repeat}\\
\begin{enumerate}
    \item Generate a solution $x'\in S$ 
    \item Calculate $\Delta f = f(x')-f(x)$
    \item If $\Delta f \leq 0,$ then \textit{Update.} $x \leftarrow x'$
    \item If $\Delta f > 0,$ then \textit{Update.} $x \leftarrow x'$ with probability $\exp(-\Delta f / t_{k})$
    \item $k \leftarrow k + 1$
\end{enumerate}
\State \textbf{until} stopping criterion is met.
\end{algorithmic}
\end{algorithm}\\
\newpage
\subsubsection{Particle Swarm Optimization}\label{PSO_s}
\paragraph{History and motivation}
Particle Swarm Optimization is an algorithm capable of minimizing a non-convex and multidimensional problem. The algorithm and its concept of "Particle Swarm Optimization (PSO)" were introduced by James Kennedy and Russel Ebhart in 1995 \cite{Kennedy1995}. As described in \cite[1]{PSO}, the algorithm is inspired by reproducing the behaviors of animals in their natural habitat, such as bird flocking or fish schooling. PSO simulates the flocking behavior of birds. The analogy is the following: each bird in a bird flock, searches for the best place to rest (the best place can be characterized by food access, water access, etc.) Birds share this information and as a result, the flock can discover the best place to rest. The algorithm starts with a finite number of initial vectors (particles) which are distributed randomly throughout the domain (search space). The algorithm is based on the following two ideas:
\begin{itemize}
    \item Each particle determines how good its current position is. The lower the function evaluation at the position of the particle is, the better its position is. The particle makes its next move according to the knowledge of its previous positions and the knowledge of the best position obtained by other particles in the swarm. 
    \item In each iteration of the algorithm, a stochastic factor determines, whether a particle moves more towards the globally found best solution or remains more in the vicinity of its own best solution. This stochasticity combined with a good initial distribution of the swarm enables for a thorough exploration of the search space and gives a high chance of finding better solutions close to the global minimum.
\end{itemize}
\paragraph{General setting}
The primary goal of the algorithm is to minimize a function $f: \mathbf{R}^n \longrightarrow \mathbf{R}.$ The algorithm starts by choosing a maximum number of particles $N$ and determines $f(x^{i}) = f(x_{1}^{i},\ldots,x_{n}^{i})$ for each particle $i \in \{1,\ldots,N\}.$ These function evaluations determine how good the positions of each particle $x^{i}$ are.
The swarm, which consists of all particles combined, is interconnected. This means that the particles exchange information, and every particle is aware of the best position that any other particle in the swarm has ever reached. For each particle $i$, its next position is obtained by adding a velocity vector to its current position. As described in \cite[2]{PSO}, the iteration reads as follows:
\begin{gather*}\tag{4.6.3.1}
x^{i}(k + 1) = x^{i}(k) + v^{i}(k+1)\hspace{3.5cm}\\
\rule{0pt}{3.5em} % Adjust the second value to change the spacing
\begin{aligned}
v^{i}(k+1) &= v^{i}(k) \hspace{6.095cm}\textit{1st component}\\ 
&+ C_{1} \cdot Rnd(0, 1) \cdot \left(pb^{i}(k) - x^{i}(k)\right) \hspace{2cm}\textit{2nd component}\\
&+ C_{2} \cdot Rnd(0, 1) \cdot \left(gb^{i}(k) - x^{i}(k)\right) \hspace{2cm}\textit{3rd component}
\end{aligned}
\end{gather*}
\textbf{Caption:}\\
\rule{0pt}{4.5em} % Adjust the second value to change the spacing
\begin{tabular}{ll}
$x^{i}(k)$ & position of particle $i$ at iteration $k$\\
$v^{i}(k)$ & velocity of particle $i$ at iteration $k$\\
$C_{1}$ & cognitive acceleration constant\\
$C_{2}$ & social acceleration constant\\
$Rnd(0, 1)$ & stochastic component of the algorithm, a random number between 0 and 1\\
$pb^{i}(k)$ & personal best position of particle $i$ at iteration $k$\\
$gb^{i}(k)$ & global best position at iteration $k$
\end{tabular}\\
\newline
\newline
Regarding the velocity update, the first component represents the momentum of the particle which is its current velocity. The second component is the cognitive component which depends heavily on the distance of the particle's current position to the best solution it has ever visited. The third component is the social component, which depends heavily on the distance of the particle's current position to the global best position obtained by any particle. At the beginning of the algorithm, the constants $C_{1}$, $C_{2}$, the size of the swarm $N$ and the maximum iteration number $k_{max}$ are initialized. Subsequently, an initial distribution of the swarm over the search space is determined randomly. Additionally, each particle $i$ is randomly assigned an initial velocity vector $v^{i}(0)$. However, to prevent large initial offsets, keeping the initial velocity vector $v^{i}(0)$ relatively low is recommended. The algorithm terminates whenever the maximum number of iterations $k_{max}$ is reached. 
























