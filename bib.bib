@book{boyd2004convex,
  title={Convex optimization},
  author={Boyd, Stephen and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge university press}
}

@book{adams2013calculus,
  title={Calculus: A Complete Course},
  author={Adams, R.A. and Essex, C.},
  isbn={9780321781079},
  url={https://books.google.nl/books?id=5aaEMAEACAAJ},
  year={2013},
  publisher={Pearson}
}

@inbook{doi:https://doi.org/10.1002/9780470173862.app3,
publisher = {John Wiley \& Sons, Ltd},
isbn = {9780470173862},
title = {Appendix C: Positive Semidefinite and Positive Definite Matrices},
booktitle = {Parameter Estimation for Scientists and Engineers},
chapter = {},
pages = {259-263},
doi = {https://doi.org/10.1002/9780470173862.app3},
year = {2007},
abstract = {Summary This chapter contains sections titled: Real Positive Semidefinite and Positive Definite Matrices Complex Positive Semidefinite and Positive Definite Matrices}
}

@online{GDlips,
  title = {Gradient-based optimization},
  author = {Jeffrey A. Fessler},
  url = {https://web.eecs.umich.edu/~fessler/course/598/l/n-03-gd.pdf},
  note = {Accessed: 16-1-2020},
}

@online{matrixnorms,
  title = {Matrix Norms},
  url = {https://www2.karlin.mff.cuni.cz/~congreve/SS2019_NUMMATH/2020_03_25.pdf},
  note = {Accessed: 25-3-2020},
}

@online{GDandLS,
  author = {Chunpai Wang},
  title = {Notes on Convex Optimization},
  url = {https://chunpai.github.io/assets/note/1__Gradient_Descent_and_Line_Search.pdf},
  year = {2016},
  note = {Accessed: 1-3-2016},
}

@article{Test_functions,
 author  = {Momin Jamil, Xin-She Yang},
 title   = {A Literature Survey of Benchmark Functions For Global Optimization Problems},
 journal = {Int. Journal of Mathematical Modelling and Numerical Optimisation},
 year    = {2013},
 volume  = {4},
 number  = {2},
 pages   = {150--194},
 month   = aug,
 url     = {https://doi.org/10.48550/arXiv.1308.4008},
 }

 @online{coursenotesML,
    author = {Peyré, Gabriel},
    title = {Optimization for machine learning},
    url = {https://mathematical-tours.github.io/book-sources/optim-ml/OptimML.pdf},
    year = {2021},
    note = {Course notes on March 30, 2021}
}

@article{murata,
author = {Murata, Noboru},
year = {1998},
month = {05},
pages = {},
title = {A Statistical Study on On-line Learning}
}

@techreport{turinici2021convergence,
  doi = {10.5281/ZENODO.4638695},
  url = {https://zenodo.org/record/4638695},
  author = {Turinici, Gabriel},
  keywords = {Stochastic Gradient Descent, Neural Network, SGD, Adam, RMSprop, Gabriel TURINICI},
  language = {en},
  title = {The convergence of the Stochastic Gradient Descent (SGD) : a self-contained proof},
  publisher = {Zenodo},
  year = {2021},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}

@book{horst1995handbook,
  title={Handbook of Global Optimization},
  editor={Horst, Reiner and Pardalos, Panos M.},
  year={1995},
  publisher={Springer Science \& Business Media},
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{XIANG1997216,
title = {Generalized simulated annealing algorithm and its application to the Thomson model},
journal = {Physics Letters A},
volume = {233},
number = {3},
pages = {216-220},
year = {1997},
issn = {0375-9601},
doi = {https://doi.org/10.1016/S0375-9601(97)00474-X},
url = {https://www.sciencedirect.com/science/article/pii/S037596019700474X},
author = {Y Xiang and D.Y Sun and W Fan and X.G Gong},
abstract = {Based on the Tsallis statistics, the generalized simulated annealing algorithm (GSA) is tested and developed. Studies on the Thomson model show that the GSA is more efficient than the classical simulated annealing and the fast simulated annealing. The fluctuation of energy is reduced drastically. The convergence to the global minimum is fast. We believe the GSA algorithm is a powerful method to find the global minimum in more realistic problems, like the equilibrium structure of big clusters.}
}

@Inbook{Henderson2003,
author="Henderson, Darrall
and Jacobson, Sheldon H.
and Johnson, Alan W.",
editor="Glover, Fred
and Kochenberger, Gary A.",
title="The Theory and Practice of Simulated Annealing",
bookTitle="Handbook of Metaheuristics",
year="2003",
publisher="Springer US",
address="Boston, MA",
pages="287--319",
abstract="Simulated annealing is a popular local search meta-heuristic used to address discrete and, to a lesser extent, continuous optimization problems. The key feature of simulated annealing is that it provides a means to escape local optima by allowing hill-climbing moves (i.e., moves which worsen the objective function value) in hopes of finding a global optimum. A brief history of simulated annealing is presented, including a review of its application to discrete and continuous optimization problems. Convergence theory for simulated annealing is reviewed, as well as recent advances in the analysis of finite time performance. Other local search algorithms are discussed in terms of their relationship to simulated annealing. The chapter also presents practical guidelines for the implementation of simulated annealing in terms of cooling schedules, neighborhood functions, and appropriate applications.",
isbn="978-0-306-48056-0",
doi="10.1007/0-306-48056-5_10",
url="https://doi.org/10.1007/0-306-48056-5_10"
}

@article{metropolis1953equation,
  title={Equation of State Calculations by Fast Computing Machines},
  author={Metropolis, Nicholas and Rosenbluth, Arianna W and Rosenbluth, Marshall N and Teller, Augusta H and Teller, Edward},
  journal={The Journal of Chemical Physics},
  volume={21},
  number={6},
  pages={1087--1092},
  year={1953},
  doi={10.1063/1.1699114},
  publisher={AIP Publishing}
}

@article{Kennedy1995,
  author       = {Kennedy, J. and Eberhart, R.},
  title        = {Particle Swarm Optimization},
  journaltitle = {Proceedings of the IEEE International Conference on Neural Networks},
  year         = {1995},
  volume       = {4},
  pages        = {1942-1948},
  doi          = {10.1109/ICNN.1995.488968},
}

@article{PSO,
author = {Pereira, Gonçalo},
year = {2011},
month = {05},
pages = {},
title = {Particle Swarm Optimization}
}

@article{deng2012mnist,
  title={The mnist database of handwritten digit images for machine learning research},
  author={Deng, Li},
  journal={IEEE Signal Processing Magazine},
  volume={29},
  number={6},
  pages={141--142},
  year={2012},
  publisher={IEEE}
}

@incollection{NEURIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@book{python,
 author = {Van Rossum, Guido and Drake, Fred L.},
 title = {Python 3 Reference Manual},
 year = {2009},
 isbn = {1441412697},
 publisher = {CreateSpace},
 address = {Scotts Valley, CA}
}

@book{bishop2006,
  title={Pattern recognition and machine learning},
  author={Bishop, Christopher M},
  year={2006},
  publisher={Springer}
}

@book{james2013,
  title={An introduction to statistical learning},
  author={James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  volume={112},
  year={2013},
  publisher={Springer}
}

@article{ThesisCode2023,
  author={Ekici, B.}, 
  title={Thesis: (Stochastic) Optimization - Code}, 
  year={2023}, 
  url={https://github.com/bekici/Bachelor_thesis_bekici} 
}
